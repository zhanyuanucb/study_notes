{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the front\n",
    "This is the note I wrote down while going through the note on [Binary Erasure Channel](./references/bec.pdf).  \n",
    "I included the following in this note:  \n",
    "- Background knowledge to briefly introduce the model of communication system designed by Shannon in 1940s.\n",
    "- Paraphrase and visualization of the concept of *Capacity of a Channel.*\n",
    "- The formal definition of channal capacity and how to get the capacity of a BEC by this definition.\n",
    "\n",
    "### Backgroud\n",
    "![](./images/1.png)\n",
    "Source message has an **input alphabet** $\\mathcal{X}$ and length *L*. The output from the noisy channel as **output alphabet** $\\mathcal{Y}$. The process of sending the source message will be:\n",
    "$$\\mathcal{X}^L -f_n-> \\mathcal{X}^n -> \\text{Noisy Channel} -> \\mathcal{Y}^n -g_n-> \\hat{\\mathcal{X}}^L$$.\n",
    "\n",
    "In this note, we are going to focus on Binary Erasure Channel, where the input has probability *p* get erased by the noisy channal.\n",
    "![](./images/2.png)\n",
    "Hence, for BEC, $\\mathcal{X} = \\{0, 1\\}$ and $\\mathcal{Y} = \\{0, e, 1\\}$.\n",
    "\n",
    "### Channel Capacity\n",
    "Channel Capacity is defined by the largest **achievable rate** of the channel.\n",
    "\n",
    "Rate is defined by $R := \\frac{L}{n}$, where L is the length of the source message and n is the length of the encoded message. When it comes to BEC, it make sense to set $n > L$ to deal with the erasure error.\n",
    "\n",
    "Achievability is expressed by $P_e(n)$, the **maximum probability of error** of a code, which is defined as:\n",
    "![](./images/3.png)\n",
    "In English, it is the maximum probability that $g_n$ gives a different result from the source message and thus it fails to decode the message. The maximum is taken over all the choices of the source message.\n",
    "A rate is achiveable implies that given this rate, $P_e(n) \\to 0 \\text{ as } n \\to \\infty$. In other words, as we send tons of message *in one time,* the receiver can always get the correct result. It is also called asymptotically error-free in this case.\n",
    "\n",
    "Little visualization of *capacity.*\n",
    "![](./images/4.jpg)\n",
    "\n",
    "### The general result of channel capacity\n",
    "The formal definition of channel capacity is expressed through the **Mutual Information** of two random variables. The following calculation refers to the concept of **Entropy** of random variables. For more details of mutual information and entropy, check the note [info_theory.pdf](./references/info_theory.pdf).\n",
    "\n",
    "![](./images/5.png)\n",
    "\n",
    "By an *oracle argument* and construction stated in the note, we've shown that the capacity of BEC is *1 - p*. Now, I want to verify this result by the formal definition of channel capacity.\n",
    "\n",
    "First, note that for BEC, $\\mathcal{X} = \\{0, 1\\}$ and $\\mathcal{Y} = \\{0, e, 1\\}$. So we have \n",
    "$$ Y|X=x =\n",
    "  \\begin{cases}\n",
    "    e       & \\quad \\text{with probability } p \\\\\n",
    "    x  & \\quad \\text{with probability } 1 - p\n",
    "  \\end{cases}\n",
    "$$\n",
    "This tells us that given our BEC, H(Y|X) is a constant. \n",
    "\n",
    "We also have $I(X;Y) = H(Y) - H(Y|X)$.\n",
    "To maximize $I(X;Y)$ over the distribution of X, the intuition is to maximize the entropy of X since large H(X) implies large H(Y). Intuition also tells me that X with equal chance of being 0 and 1 gives the largest entropy. Try to show it if you don't believe it.  \n",
    "Hence, we have\n",
    "$$ Y =\n",
    "  \\begin{cases}\n",
    "    1       & \\quad \\text{with probability } \\frac{1-p}{2} \\\\\n",
    "    e  & \\quad \\text{with probability } p \\\\\n",
    "    0  & \\quad \\text{with probability } \\frac{1-p}{2}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "Now I'm ready to do the calculation:\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(Y) &= - (\\frac{1-p}{2}log_2(\\frac{2}{1-p}) + plog_2(p) \\frac{1-p}{2}log_2(\\frac{2}{1-p})) \\\\\n",
    "     &= (1-p)[log_2(1-p) - 1] + plog_2(p) \\\\\n",
    "\\\\\n",
    "H(Y|X) &= -(plog_2(\\frac{1}{p}) + (1-p)log_2(\\frac{1}{1-p})) \\\\\n",
    "       &= log_2(p) + (1-p)log_2(1-p) \\\\\n",
    "\\\\\n",
    "I(X;Y) &= H(Y) - H(Y|X) \\\\\n",
    "       &= 1-p, \\text{ as desired.}\n",
    "\\end{align*}\n",
    "$$\n",
    "Sweet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
