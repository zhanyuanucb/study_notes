{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation -- the Workhourse of ML\n",
    "\n",
    "### On the front\n",
    "I took this note when I was studying the backpropagation algorithm from [ad-n-nn.pdf](./references/ad-n-nn.pdf). I was trapped by being too focus on the which layout should I use for the chain rule in backprop, which blocked me away from see the general result by the autometic differentiation. The note provided a new perspective and wiped out my confusion.    \n",
    "This note is also a derivative of another note of mine [backprop_in_rnn](./backprop_in_rnn.ipynb) and tries to further justify the calculation of gradiant in Siraj's implement of RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As usual, let's take a concrete example:  \n",
    "Consider `dWhy += np.dot(dy, hs[t].T)`. `dWhy` is the gradiant w.r.t `Why`, or $\\frac{\\partial l_t}{\\partial W_{hy}}$.  \n",
    "\n",
    "For a concrete example, let:  \n",
    "`Why` = $W_{hy}$ = $\\begin{bmatrix}\n",
    "       w_{11} & w_{12} & w_{13}           \\\\[0.3em]\n",
    "       w_{21} & w_{22} & w_{23}\n",
    "     \\end{bmatrix}$\n",
    "  \n",
    " `hs[t]` = $H_t$ = $\\begin{bmatrix}\n",
    "       h_{1}\\\\[0.3em]\n",
    "       h_{2}\\\\[0.3em]\n",
    "       h_{3}\n",
    "     \\end{bmatrix}$\n",
    "  \n",
    "  `by` = $\\begin{bmatrix}\n",
    "       b_{1}\\\\[0.3em]\n",
    "       b_{2}\n",
    "     \\end{bmatrix}$\n",
    "\n",
    "Now, we have `ys[t] = np.dot(Why, hs[t]) + by`:  \n",
    "`ys[t]` = $Y_t$ = $\\begin{bmatrix}\n",
    "      w_{11}h_1 + w_{12}h_2 + w_{13}h_3 + b_1\\\\[0.3em]\n",
    "      w_{21}h_1 + w_{22}h_2 + w_{23}h_3 + b_2\n",
    "     \\end{bmatrix}\n",
    "        = \\begin{bmatrix}\n",
    "       y_{1}\\\\[0.3em]\n",
    "       y_{2}\n",
    "     \\end{bmatrix}$\n",
    "     \n",
    "So, `ps[t]` = $\\begin{bmatrix}\n",
    "       \\frac{e^{y_1}}{e^{y_1} + e^{y_2}} \\\\[0.3em]\n",
    "       \\frac{e^{y_2}}{e^{y_1} + e^{y_2}}\n",
    "     \\end{bmatrix}\n",
    "      = \\begin{bmatrix}\n",
    "       p_{1}\\\\[0.3em]\n",
    "       p_{2}\n",
    "     \\end{bmatrix}$\n",
    "\n",
    "**Loss function:**  \n",
    "In the code, the loss at the $t^{th}$ iteration is defined by `-np.log(ps[t][targets[t],0])`, the negative log-likelihood if the model gives the right answer.\n",
    "\n",
    "\n",
    "WLOG, let's just assume that at this iteration, the second letter is the target (or the ground truth). So, set `loss` = $l_t$ = $-log(p_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by *ad-n-nn.pdf*, I draw the following expression graph:\n",
    "![](./images/exp-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sidenote: this [article](https://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/) elaborates the connection between backprop and the method of Lagrange multipliers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backprop algorithm is as followed: \n",
    "![screenshot from the note](./images/bp.png)\n",
    "Writting the forwardpass in the expression graph fashion shows the dependencies among inputs, intermedia variables, and output. This will help us to observe backprop closer:  \n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial p_1} = 0 \\quad &\\frac{\\partial L}{\\partial p_2} = -\\frac{1}{p_2} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial y_1} = \\frac{\\partial L}{\\partial p_1}\\frac{\\partial p_1}{\\partial y_1} + \\frac{\\partial L}{\\partial p_2}\\frac{\\partial p_2}{\\partial y_1} = p_1 \\quad &\\frac{\\partial L}{\\partial y_2} = \\frac{\\partial L}{\\partial p_1}\\frac{\\partial p_1}{\\partial y_2} + \\frac{\\partial L}{\\partial p_2}\\frac{\\partial p_2}{\\partial y_2} = p_2 - 1 \\\\\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial y_1}\\frac{\\partial y_1}{\\partial z_1} = p_1 \\quad &\\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial y_2}\\frac{\\partial y_2}{\\partial z_2} = p_2 - 1\\\\\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial \\textbf{w}_1} = \\frac{\\partial L}{\\partial z_1}\\frac{\\partial z_1}{\\partial \\textbf{w}_1}=p_1[\\begin{smallmatrix} h_1 & h_2 & h_3 \\end{smallmatrix}] \\quad &\\frac{\\partial L}{\\partial \\textbf{w}_2} = \\frac{\\partial L}{\\partial z_2}\\frac{\\partial z_2}{\\partial \\textbf{w}_2} = (p_2 - 1)[\\begin{smallmatrix} h_1 & h_2 & h_3 \\end{smallmatrix}]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "*Sidenote: Given that* $ z_1 = \\textbf{w}_1 \\textbf{h} $, *show* $\\frac{\\partial z_1}{\\partial \\textbf{w}_1} = [\\begin{smallmatrix} h_1 & h_2 & h_3 \\end{smallmatrix}]$:  \n",
    "$\\begin{align*}\n",
    "& \\text{Let } \\textbf{v} = \\textbf{w}_1 \\bigotimes \\textbf{h} = \\bigg[ \\begin{smallmatrix} w_{11}h_1 \\\\ w_{12}h_2 \\\\ w_{13}h_3 \\end{smallmatrix} \\bigg], \\text{ then } z_1 = [\\begin{smallmatrix} 1 & 1 & 1 \\end{smallmatrix}] \\textbf{v} \\\\\n",
    "\\\\\n",
    "& \\text{Now, } \\frac{\\partial z_1}{\\partial \\textbf{v}} = [\\begin{smallmatrix} 1 & 1 & 1 \\end{smallmatrix}] \\text{, and } \\frac{\\partial \\textbf{v}}{\\partial \\textbf{w}_1} \\bigg[ \\begin{smallmatrix} h_1 & 0 & 0 \\\\ 0 & h_2 & 0 \\\\ 0 & 0 & h_3 \\end{smallmatrix} \\bigg] \\\\\n",
    "\\\\\n",
    "&\\text{By chain rule, }\\frac{\\partial z_1}{\\partial \\textbf{w}_1} = \\frac{\\partial z_1}{\\partial \\textbf{v}}\\frac{\\partial \\textbf{v}}{\\partial \\textbf{w}_1} = [\\begin{smallmatrix} h_1 & h_2 & h_3 \\end{smallmatrix}]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorder what we got: $\\frac{\\partial L}{\\partial W_{hy}} =  \\bigg[ \\begin{smallmatrix} \\frac{\\partial L}{\\partial \\textbf{w}_1} \\\\ \\frac{\\partial L}{\\partial \\textbf{w}_2}\\end{smallmatrix} \\bigg] = \\big[ \\begin{smallmatrix} p_1[\\begin{smallmatrix} h_1 & h_2 & h_3 \\end{smallmatrix}]\n",
    "\\\\(p_2 - 1)[\\begin{smallmatrix} h_1 & h_2 & h_3 \\end{smallmatrix}]\\end{smallmatrix} \\big] = \\big[ \\begin{smallmatrix} p_1 \\\\ p_2 - 1\\end{smallmatrix} \\big]$ $[ \\begin{smallmatrix} h_1 & h_2 & h_3 \\end{smallmatrix} ]$.\n",
    "\n",
    "Again, we have the same result as the code `Why += np.dot(np.dot(dy, hs[t].T))` from different angle to see it, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punchline\n",
    "To calculate $\\frac{\\partial L}{\\partial W_{hy}}$, backprop algorithm only needs to look forward one step to get the gradiant of $W_{hy}$'s child, which is $\\big[ \\begin{smallmatrix} z_1 \\\\ z_2\\end{smallmatrix} \\big]$. The great thing is that by backprop, the gradiant of $\\big[ \\begin{smallmatrix} z_1 \\\\ z_2\\end{smallmatrix} \\big]$ is what we already know if we are trying to calculate $\\frac{\\partial L}{\\partial W_{hy}}$. Therefore, we only need to calculate $\\frac{\\partial z_1}{\\partial \\textbf{w}_1}$ and $\\frac{\\partial z_2}{\\partial \\textbf{w}_2}$. What's even better is that $\\frac{\\partial z_1}{\\partial \\textbf{w}_1}$ and $\\frac{\\partial z_2}{\\partial \\textbf{w}_2}$ share the same result (this is generally true for dot product) and indeed, we only need one of them to compute $\\frac{\\partial L}{\\partial W_{hy}}$. **Backprop has the same time complexity as the forward propagation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the generality of the result on calculating any the weights of a fully connected layers, note that besides `dWhy += np.dot(np.dot(dy, hs[t].T))`, we also have `dWxh += np.dot(dhraw, xs[t].T)` and `dWhh += np.dot(dhraw, hs[t-1].T)`, where `dhraw` is the gradiant of the child of both `Wxh` and `Whh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we miss `dhnext = np.dot(Whh.T, dhraw)` and `dh = np.dot(Why.T, dy) + dhnext`. However, they are the same under the hood. Take this example to see why they look different:\n",
    "\n",
    "$Z_t = W_{hy}H_t$ = $\\begin{bmatrix}\n",
    "       w_{11} & w_{12} & w_{13}           \\\\[0.3em]\n",
    "       w_{21} & w_{22} & w_{23}\n",
    "     \\end{bmatrix}$ $\\begin{bmatrix}\n",
    "       h_{1}\\\\[0.3em]\n",
    "       h_{2}\\\\[0.3em]\n",
    "       h_{3}\n",
    "     \\end{bmatrix}$. As you may notice, one is on the left and the other is on the right."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
