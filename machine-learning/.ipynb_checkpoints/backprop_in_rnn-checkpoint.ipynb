{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in RNN and Its Realization with Numpy\n",
    "I took this note while studying Siraj's code in his [RNN.ipynb](https://github.com/llSourcell/recurrent_neural_network/blob/master/RNN.ipynb). \n",
    "\n",
    "I spent quite a while in understanding the backpropagation part of the code:\n",
    "![](./images/1.png)\n",
    "\n",
    "This note tries to answer the following two questions about the code above:\n",
    "1. `dh = np.dot(Why.T, dy) + dhnext`. What's `dh` and why `dhnext`?\n",
    "2. Those transpose operations in the code seems wierd since I thought chain rule of matrix calculus doesn't involve any transpose (but we'll see). Why and how do they work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good to know before the code\n",
    "\n",
    "- **Recurent Neural Network:** Screenshot from Siraj's notebook \n",
    "![](./images/3.png)\n",
    "\n",
    "\n",
    "- **Layouts of matrix calculus:** here we use numerator-layout\n",
    "Matrix differentiate of two matrix $\\frac{\\partial A}{\\partial B}$ is merely differentiating each function in matrix A with respect to each variable in matrix B. Then people put the results in some nice matrix form. Usually there are two such matrix forms (a.k.a. layouts): numerator-layout and denominator-layout. They are transpose of each other. For more details, check out the following diagram from wiki:\n",
    "![](./images/2.png)\n",
    "  \n",
    "\n",
    "- **Chain rule's formula**\n",
    "With consistent matrix layout and each bold letter is a vector:  \n",
    "$\\frac{\\partial \\textbf{g}(\\textbf{u})}{\\partial x} = \\frac{\\partial \\textbf{g}(\\textbf{u})}{\\partial \\textbf{u}} \\frac{\\partial \\textbf{u}}{\\partial x}$  \n",
    "$\\frac{\\partial \\textbf{f}(\\textbf{g}(\\textbf{u}))}{\\partial x} = \\frac{\\partial \\textbf{f}(\\textbf{g})}{\\partial \\textbf{g}} \\frac{\\partial \\textbf{g}(\\textbf{u})}{\\partial \\textbf{u}} \\frac{\\partial \\textbf{u}}{\\partial x}$  \n",
    "\n",
    "**However, there is no consistant expression for chain rule when it comes to matrix-valued functions of matrices.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: what are `dh` and `dhnext`?\n",
    "The `dh` is the grandiant of the loss with respect to the matrix **hs[t]**.\n",
    "And the `dhnext` is the grandiant with respect to the matrix **hs[t-1]** *at the current iteration t*. By the way, `np.dot(Why.T, dy)` is the grandiant w.r.t **hs[t]** *at the current iteration t*. Take some time to distinguish it from `dh`.\n",
    "\n",
    "Inspect how `dh` is updated: `dh = np.dot(Why.T, dy) + dhnext`. The way `dh` gets updated is up to the recurrent structure of the model and the way we calculate the loss: the final loss value is given by the sum of loss values of all the iterations. The recurrent structure of the model implies that the **hs[t-1]** contributes to the loss at both the $(t - 1)^{th}$ and the $t^{th}$ iterations (except the last **hs[t]**. It only contributes to the loss of the last iteration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: why transpose?\n",
    "It turns out that applying chain rule on matrix differentiation does require transpose in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider `dWhy += np.dot(dy, hs[t].T)`. `dWhy` is the gradiant w.r.t `Why`, or $\\frac{\\partial l_t}{\\partial W_{hy}}$.  \n",
    "\n",
    "For a concrete example, let:  \n",
    "`Why` = $W_{hy}$ = $\\begin{bmatrix}\n",
    "       w_{11} & w_{12} & w_{13}           \\\\[0.3em]\n",
    "       w_{21} & w_{22} & w_{23}\n",
    "     \\end{bmatrix}$\n",
    "  \n",
    " `hs[t]` = $H_t$ = $\\begin{bmatrix}\n",
    "       h_{1}\\\\[0.3em]\n",
    "       h_{2}\\\\[0.3em]\n",
    "       h_{3}\n",
    "     \\end{bmatrix}$\n",
    "  \n",
    "  `by` = $\\begin{bmatrix}\n",
    "       b_{1}\\\\[0.3em]\n",
    "       b_{2}\n",
    "     \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have `ys[t] = np.dot(Why, hs[t]) + by`:  \n",
    "`ys[t]` = $Y_t$ = $\\begin{bmatrix}\n",
    "      w_{11}h_1 + w_{12}h_2 + w_{13}h_3 + b_1\\\\[0.3em]\n",
    "      w_{21}h_1 + w_{22}h_2 + w_{23}h_3 + b_2\n",
    "     \\end{bmatrix}\n",
    "        = \\begin{bmatrix}\n",
    "       y_{1}\\\\[0.3em]\n",
    "       y_{2}\n",
    "     \\end{bmatrix}$\n",
    "     \n",
    "So, `ps[t]` = $\\begin{bmatrix}\n",
    "       \\frac{e^{y_1}}{e^{y_1} + e^{y_2}} \\\\[0.3em]\n",
    "       \\frac{e^{y_2}}{e^{y_1} + e^{y_2}}\n",
    "     \\end{bmatrix}\n",
    "      = \\begin{bmatrix}\n",
    "       p_{1}\\\\[0.3em]\n",
    "       p_{2}\n",
    "     \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function:**  \n",
    "In the code, the loss at the $t^{th}$ iteration is defined by `-np.log(ps[t][targets[t],0])`, the negative log-likelihood if the model gives the right answer.\n",
    "\n",
    "\n",
    "WLOG, let's just assume that at this iteration, the second letter is the target (or the ground truth). So, set `loss` = $l_t$ = $-log(p_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate $\\frac{\\partial l_t}{\\partial W_{hy}}$.\n",
    "If we do the sanity check of the dimension, $\\frac{\\partial l_t}{\\partial W_{hy}}$ should have a 3x2 dimension.  Intuitively, applying chain rule means we want to find $\\frac{\\partial l_t}{\\partial Y_t} \\frac{\\partial Y_t}{\\partial W_{hy}}$. However, $\\frac{\\partial Y_t}{\\partial W_{hy}}$ is a vector-by-matrix case, I was not 100% sure how to deal with the shape of the resulting matrix. Therefore, I can't apply chain rule here like using it on scalars or vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out what's the right way to calculate the gradiant, I first found that  \n",
    "$\\frac{\\partial l_t}{\\partial w_{11}} = \\frac{\\partial l_t}{\\partial Y_t} \\frac{\\partial Y_t}{\\partial w_{11}} = [\\begin{smallmatrix}\n",
    "p_1 & p_2 - 1\n",
    "\\end{smallmatrix}] \\big[ \\begin{smallmatrix} h_1 \\\\ \n",
    "0 \\end{smallmatrix} \\big] = p_1h_1$\n",
    "\n",
    "Generalize this result to the rest of the entries and get the Jacobian $\\frac{\\partial l_t}{\\partial W_{hy}} = \\big[ \\begin{smallmatrix} p_1 \\\\ p_2 - 1\\end{smallmatrix} \\big]$ $[ \\begin{smallmatrix} h_1 & h_2 & h_3 \\end{smallmatrix} ]$, which is exactly `np.dot(np.dot(dy, hs[t].T))`. \n",
    "\n",
    "Conclusion: we do need the transpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to generalize this result?\n",
    "People in industry can't be just code up the chain rule in backprop matrix by matrix. Indeed, we should jump out of the constraint of layout and find a way to automate the backprop. This has something to do with automatic differentiation (AD).  \n",
    "Check this note I left when study some resources of AD and backprop algorithm --> [the_heart_of_backprop.ipynb](./the_heart_of_backprop.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful resources:\n",
    "- [matrix calculus from wiki](https://en.wikipedia.org/wiki/Matrix_calculus)\n",
    "- [matrix calculus for deep learning](https://explained.ai/matrix-calculus/index.html)\n",
    "- [backprop is not just the chain rule](https://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# YOLO: You Only want to Look Once -- The whole forwardpass\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a concrete example, let:  \n",
    "`Wxh` = $\\begin{bmatrix}\n",
    "       w_{11} & w_{12} & w_{13}           \\\\[0.3em]\n",
    "       w_{21} & w_{22} & w_{23}\n",
    "     \\end{bmatrix}$\n",
    "  \n",
    "`xs[t]` = $\\begin{bmatrix}\n",
    "       x_{1}\\\\[0.3em]\n",
    "       x_{2}\\\\[0.3em]\n",
    "       x_{3}\n",
    "     \\end{bmatrix}$\n",
    "\n",
    "`Whh` = $\\begin{bmatrix}\n",
    "       v_{11} & v_{12} & v_{13}           \\\\[0.3em]\n",
    "       v_{21} & v_{22} & v_{23}\n",
    "     \\end{bmatrix}$\n",
    "\n",
    "`hs[t-1]` = $\\begin{bmatrix}\n",
    "       h_{1}\\\\[0.3em]\n",
    "       h_{2}\\\\[0.3em]\n",
    "       h_{3}\n",
    "     \\end{bmatrix}$\n",
    "\n",
    "`bh` = $\\begin{bmatrix}\n",
    "       b_{1}\\\\[0.3em]\n",
    "       b_{2}\n",
    "     \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have `hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)`\n",
    "\n",
    "`hs[t]` = $H_t$ = $\\begin{bmatrix}\n",
    "      w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + v_{11}h_1 + v_{12}h_2 + v_{13}h_3 + b_1\\\\[0.3em]\n",
    "      w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + v_{21}h_1 + v_{22}h_2 + v_{23}h_3 + b_2\n",
    "     \\end{bmatrix}\n",
    "        = \\begin{bmatrix}\n",
    "       h_{1}\\\\[0.3em]\n",
    "       h_{2}\n",
    "     \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let `Why` = $W_{hy}$ = $\\begin{bmatrix}\n",
    "       u_{11} & u_{12} & u_{13}           \\\\[0.3em]\n",
    "       u_{21} & u_{22} & u_{23}\n",
    "     \\end{bmatrix}$\n",
    "  \n",
    "  `by` = $\\begin{bmatrix}\n",
    "       s_{1}\\\\[0.3em]\n",
    "       s_{2}\n",
    "     \\end{bmatrix}$\n",
    "\n",
    "Now, we have `ys[t] = np.dot(Why, hs[t]) + by`:  \n",
    "`ys[t]` = $Y_t$ = $\\begin{bmatrix}\n",
    "      u_{11}h_1 + u_{12}h_2 + u_{13}h_3 + s_1\\\\[0.3em]\n",
    "      u_{21}h_1 + u_{22}h_2 + u_{23}h_3 + s_2\n",
    "     \\end{bmatrix}\n",
    "        = \\begin{bmatrix}\n",
    "       y_{1}\\\\[0.3em]\n",
    "       y_{2}\n",
    "     \\end{bmatrix}$\n",
    "\n",
    "So, `ps[t]` = $\\begin{bmatrix}\n",
    "       \\frac{e^{y_1}}{e^{y_1} + e^{y_2}} \\\\[0.3em]\n",
    "       \\frac{e^{y_2}}{e^{y_1} + e^{y_2}}\n",
    "     \\end{bmatrix}\n",
    "      = \\begin{bmatrix}\n",
    "       p_{1}\\\\[0.3em]\n",
    "       p_{2}\n",
    "     \\end{bmatrix}$\n",
    "\n",
    "WLOG, let's just set `loss` = $l_t$ = $-log(p_2)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
